{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ouvG2EivvSY"
      },
      "source": [
        "# Uploading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqSlqXQnvpf0",
        "outputId": "3e575d81-e075-4db3-9d0f-cbfeb4b6f893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5bwfTgxvuUK",
        "outputId": "d7820086-b16b-4888-f17d-6cf2db623094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 2205\n",
            "Testing set size: 552\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Set the path to your dataset\n",
        "#dataset_path = '/content/Training set' # images\n",
        "dataset_path = '/Users/macbookair/Desktop/541_model/input/Images' # images\n",
        "\n",
        "# Get a list of all image files in the dataset\n",
        "image_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith('.jpg')]\n",
        "\n",
        "# Split the image files into training and testing sets\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Print the number of images in each set\n",
        "print(f'Training set size: {len(train_files)}')\n",
        "print(f'Testing set size: {len(test_files)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jyCO6DCNUWF",
        "outputId": "bf650aba-af19-4e64-ac1f-2036d99de380"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying files: 2757 files [00:17, 156.57 files/s]\n"
          ]
        }
      ],
      "source": [
        "#!pip install split-folders\n",
        "import splitfolders\n",
        "\n",
        "\n",
        "input_folder = '/Users/macbookair/Desktop/541_model/input'\n",
        "\n",
        "\n",
        "# Split with a ratio.\n",
        "# To only split into training and validation set, set a tuple to `ratio`, i.e, (.8, .2).\n",
        "# Train, val, test\n",
        "splitfolders.ratio(input_folder,\n",
        "                   output=\"Images_for_train_test\",\n",
        "                   seed=1337,\n",
        "                   ratio=(.8, .2),\n",
        "                   group_prefix=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9NlNXOzv2RW"
      },
      "source": [
        "# Downloading pretrained tensor flow model for detection of cards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSQQRLeRNf-4"
      },
      "outputs": [],
      "source": [
        "from pandas.core.dtypes.common import is_array_like\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "# Download the pre-trained SSD model\n",
        "model_url = 'http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz'\n",
        "model_file = 'ssd_mobilenet_v1_coco_2018_01_28/frozen_inference_graph.pb'\n",
        "\n",
        "urllib.request.urlretrieve(model_url, './model.tar.gz')\n",
        "import tarfile\n",
        "with tarfile.open('./model.tar.gz', 'r') as tar:\n",
        "    tar.extractall('./')\n",
        "\n",
        "# Load the pre-trained SSD model\n",
        "model_path = './' + model_file\n",
        "with tf.io.gfile.GFile(model_path, 'rb') as f:\n",
        "    graph_def = tf.compat.v1.GraphDef()\n",
        "    graph_def.ParseFromString(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEh_YnsuvroY"
      },
      "outputs": [],
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "    sess.graph.as_default()\n",
        "    tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "    # Get the input and output tensors for the model\n",
        "    input_tensor = sess.graph.get_tensor_by_name('image_tensor:0')\n",
        "    detection_boxes = sess.graph.get_tensor_by_name('detection_boxes:0')\n",
        "    detection_scores = sess.graph.get_tensor_by_name('detection_scores:0')\n",
        "    detection_classes = sess.graph.get_tensor_by_name('detection_classes:0')\n",
        "    num_detections = sess.graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "    # Set up the webcam\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    while (cap.isOpened()): # while True (prior)\n",
        "        # Capture a frame from the webcam\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if frame is None:\n",
        "            continue\n",
        "\n",
        "        # Prepare the frame for input to the model\n",
        "        frame_expanded = np.expand_dims(frame, axis=0)\n",
        "\n",
        "        # Perform object detection on the frame\n",
        "        (boxes, scores, classes, num) = sess.run(\n",
        "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "            feed_dict={input_tensor: frame_expanded})\n",
        "\n",
        "        # Draw bounding boxes around the detected objects\n",
        "        for i in range(int(num[0])):\n",
        "            score = scores[0][i]\n",
        "            if score < 0.5:\n",
        "                continue\n",
        "            class_name = classes[0][i]\n",
        "            box = boxes[0][i]\n",
        "            ymin, xmin, ymax, xmax = box\n",
        "            x, y, w, h = int(xmin * frame.shape[1]), int(ymin * frame.shape[0]), \\\n",
        "                         int((xmax - xmin) * frame.shape[1]), int((ymax - ymin) * frame.shape[0])\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, f'{class_name} ({score:.2f})', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # Show the frame with the detections\n",
        "        cv2.imshow('Object Detection', frame)\n",
        "\n",
        "        # Check for exit key\n",
        "        if cv2.waitKey(1) == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release the webcam and close\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FltFDNYwv-oO"
      },
      "source": [
        "# Finding performance of test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "XFqaOerYvrq1",
        "outputId": "03c5df44-2792-4fbe-dcf0-317cb77c8389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2205 images belonging to 1 classes.\n",
            "Found 552 images belonging to 1 classes.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Layer \"dense_3\" expects 1 input(s), but it received 4 input tensors. Inputs received: [<tf.Tensor 'Placeholder:0' shape=(None, 100, 4) dtype=float32>, <tf.Tensor 'Placeholder_1:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'Placeholder_3:0' shape=(None,) dtype=int32>]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c1e3aea3f69c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Replace the output layer to match the number of classes in your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0moutput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer \"dense_3\" expects 1 input(s), but it received 4 input tensors. Inputs received: [<tf.Tensor 'Placeholder:0' shape=(None, 100, 4) dtype=float32>, <tf.Tensor 'Placeholder_1:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(None, 100) dtype=float32>, <tf.Tensor 'Placeholder_3:0' shape=(None,) dtype=int32>]"
          ]
        }
      ],
      "source": [
        "#!pip install yolov3-tf2\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "from yolov3_tf2.models import YoloV3\n",
        "from keras.layers import Flatten\n",
        "\n",
        "# Set up paths and parameters\n",
        "train_dir = '/Users/macbookair/Desktop/541_model/Images_for_train_test/train' # \"/content/Training set\"\n",
        "test_dir = '/Users/macbookair/Desktop/541_model/Images_for_train_test/val' # \"/content/test set\"\n",
        "img_height, img_width = 416, 416\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load the training and testing datasets using the ImageDataGenerator class in Keras\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Load the pre-trained model (e.g. YOLOv3)\n",
        "model = YoloV3()\n",
        "\n",
        "# Freeze the convolutional layers to avoid overfitting\n",
        "for layer in model.layers:\n",
        "    if \"conv\" in layer.name:\n",
        "        layer.trainable = False\n",
        "\n",
        "# Replace the output layer to match the number of classes in your dataset\n",
        "num_classes = len(train_generator.class_indices)\n",
        "output_tensor = model.output[0]  # extract the first tensor from the list\n",
        "output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(output_tensor)\n",
        "model = tf.keras.models.Model(model.input, output_layer)\n",
        "\n",
        "\n",
        "# Compile the model with an appropriate loss function, optimizer and metrics\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "#history = model.fit(\n",
        "#        train_generator,\n",
        "#        epochs=epochs,\n",
        "#        validation_data=test_generator)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "#score = model.evaluate(test_generator, verbose=0)\n",
        "#print('Test loss:', score[0])\n",
        "#print('Test accuracy:', score[1])\n",
        "\n",
        "# Plot the accuracy and loss over time during training\n",
        "#plt.plot(history.history['accuracy'])\n",
        "#plt.plot(history.history['val_accuracy'])\n",
        "#plt.title('Model accuracy')\n",
        "#plt.ylabel('Accuracy')\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.legend(['Train', 'Test'], loc='upper left')\n",
        "#plt.show()\n",
        "\n",
        "#plt.plot(history.history['loss'])\n",
        "#plt.plot(history.history['val_loss'])\n",
        "#plt.title('Model loss')\n",
        "#plt.ylabel('Loss')\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.legend(['Train', 'Test'], loc='upper left')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJJSibEowFOB"
      },
      "source": [
        "# Card Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_-wAFB-vrtd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('card_detection_model.h5')\n",
        "\n",
        "# Define the classes of objects that the model can detect\n",
        "classes = ['Ace', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'Jack', 'Queen', 'King']\n",
        "\n",
        "# Set up the video capture from the webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Set up the video writer to save the output video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 480))\n",
        "\n",
        "while(True):\n",
        "    # Capture a frame from the webcam\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Preprocess the image for the model\n",
        "    img = cv2.resize(frame, (224, 224))\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = img / 255.0\n",
        "\n",
        "    # Run the model inference\n",
        "    prediction = model.predict(img)[0]\n",
        "\n",
        "    # Find the class with the highest probability\n",
        "    class_index = np.argmax(prediction)\n",
        "    class_name = classes[class_index]\n",
        "\n",
        "    # Draw a bounding box around the detected card\n",
        "    cv2.rectangle(frame, (50, 50), (200, 250), (0, 255, 0), 2)\n",
        "    cv2.putText(frame, class_name, (50, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "    # Show the frame on the screen\n",
        "    cv2.imshow('frame', frame)\n",
        "\n",
        "    # Write the frame to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "    # Stop the program if the 'q' key is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the video capture and writer\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Close all windows\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGMCjuAKwLZg"
      },
      "source": [
        "# Using webcam for real time simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "Iw4n-aCKvrwQ",
        "outputId": "1f85f1f8-1e8f-4ea5-9e31-2bed2609d367"
      },
      "outputs": [
        {
          "ename": "TclError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c7425d844638>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create a GUI window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Playing Card Detection\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2268\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2269\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2272\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import tkinter as tk\n",
        "from PIL import Image, ImageTk\n",
        "\n",
        "# Create a GUI window\n",
        "root = tk.Tk()\n",
        "root.title(\"Playing Card Detection\")\n",
        "\n",
        "# Set up a canvas for displaying the webcam feed\n",
        "canvas = tk.Canvas(root, width=640, height=480)\n",
        "canvas.pack()\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('card_detection_model.h5') # ... # Your trained model here\n",
        "\n",
        "# Define the function for processing the webcam feed and detecting playing cards\n",
        "def detect_cards():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Process the image frame using the trained model to detect playing cards\n",
        "        detected_cards = model.detect(frame)\n",
        "\n",
        "        # Draw rectangles around the detected cards on the frame\n",
        "        for card in detected_cards:\n",
        "            cv2.rectangle(frame, (card.xmin, card.ymin), (card.xmax, card.ymax), (0, 255, 0), 2)\n",
        "\n",
        "        # Convert the image frame to a format that can be displayed in the GUI\n",
        "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        img = Image.fromarray(img)\n",
        "        img = ImageTk.PhotoImage(image=img)\n",
        "\n",
        "        # Update the canvas with the new image\n",
        "        canvas.create_image(0, 0, anchor=tk.NW, image=img)\n",
        "        canvas.update()\n",
        "\n",
        "        # Exit the loop if the user presses the \"q\" key\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release the webcam and destroy the GUI window\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Call the detect_cards function to start processing the webcam feed\n",
        "detect_cards()\n",
        "\n",
        "# Run the GUI loop\n",
        "root.mainloop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpeF1-w-vry1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK8hnsjcvr1j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPU3ZOOfvr35"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}